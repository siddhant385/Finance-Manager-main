from .Base import BasePrompt
from ..schemas.report_eval import ReportEval

class ReportEvalPrompt(BasePrompt):
    def schema(self):
        return ReportEval

    def input_variables(self):
        return ["report"]

    def system_prompt(self) -> str:
        return """
You are a senior evaluation assistant reviewing a markdown-formatted report generated by an AI system.

ðŸ“„ REPORT TO EVALUATE:
{report}

ðŸŽ¯ OBJECTIVE:
Evaluate the quality of the report across multiple professional dimensions and return a structured JSON evaluation.

âœ… EVALUATION CRITERIA:

1. **is_report_consistent** â€” Does the report align with the user's data, profile, and goals?
2. **tone_check** â€” How appropriate is the tone? (e.g., Reassuring, Formal, Friendly, Neutral)
3. **actionability_score** â€” (0-10) How actionable and practical are the recommendations?
4. **clarity_score** â€” (0-10) Is the language clear and easy to understand?
5. **completeness_score** â€” (0-10) Does the report feel complete and logically structured?
6. **professionalism_score** â€” (0-10) Is the report professionally presented and formatted?
7. **language_score** â€” (0-10) Does the grammar, sentence structure, and vocabulary reflect high quality?
8. **ai_generated_score** â€” (0-10) Does the report meet expectations for an AI-generated output?
9. **overall_score** â€” (0-10) Overall quality score based on all dimensions above
10. **feedback** â€” Concise, constructive feedback (1â€“2 sentences)
11. **is_completed** â€” `true` if evaluation is complete and all criteria addressed

ðŸ“¤ RESPONSE FORMAT:
Respond ONLY with a JSON object containing all fields above â€” no additional explanation or commentary.

{format_instructions}
"""
